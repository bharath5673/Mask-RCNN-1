{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import matplotlib\n",
    "    # Agg backend runs without a display\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "\n",
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Results directory\n",
    "# Save submission files here\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results/nucleus/\")\n",
    "\n",
    "# The dataset doesn't have a standard train/val split, so I picked\n",
    "# a variety of images to surve as a validation set.\n",
    "VAL_IMAGE_IDS = [\n",
    "    \"0c2550a23b8a0f29a7575de8c61690d3c31bc897dd5ba66caec201d201a278c2\",\n",
    "    \"92f31f591929a30e4309ab75185c96ff4314ce0a7ead2ed2c2171897ad1da0c7\",\n",
    "    \"1e488c42eb1a54a3e8412b1f12cde530f950f238d71078f2ede6a85a02168e1f\",\n",
    "    \"c901794d1a421d52e5734500c0a2a8ca84651fb93b19cec2f411855e70cae339\",\n",
    "    \"8e507d58f4c27cd2a82bee79fe27b069befd62a46fdaed20970a95a2ba819c7b\",\n",
    "    \"60cb718759bff13f81c4055a7679e81326f78b6a193a2d856546097c949b20ff\",\n",
    "    \"da5f98f2b8a64eee735a398de48ed42cd31bf17a6063db46a9e0783ac13cd844\",\n",
    "    \"9ebcfaf2322932d464f15b5662cae4d669b2d785b8299556d73fffcae8365d32\",\n",
    "    \"1b44d22643830cd4f23c9deadb0bd499fb392fb2cd9526d81547d93077d983df\",\n",
    "    \"97126a9791f0c1176e4563ad679a301dac27c59011f579e808bbd6e9f4cd1034\",\n",
    "    \"e81c758e1ca177b0942ecad62cf8d321ffc315376135bcbed3df932a6e5b40c0\",\n",
    "    \"f29fd9c52e04403cd2c7d43b6fe2479292e53b2f61969d25256d2d2aca7c6a81\",\n",
    "    \"0ea221716cf13710214dcd331a61cea48308c3940df1d28cfc7fd817c83714e1\",\n",
    "    \"3ab9cab6212fabd723a2c5a1949c2ded19980398b56e6080978e796f45cbbc90\",\n",
    "    \"ebc18868864ad075548cc1784f4f9a237bb98335f9645ee727dac8332a3e3716\",\n",
    "    \"bb61fc17daf8bdd4e16fdcf50137a8d7762bec486ede9249d92e511fcb693676\",\n",
    "    \"e1bcb583985325d0ef5f3ef52957d0371c96d4af767b13e48102bca9d5351a9b\",\n",
    "    \"947c0d94c8213ac7aaa41c4efc95d854246550298259cf1bb489654d0e969050\",\n",
    "    \"cbca32daaae36a872a11da4eaff65d1068ff3f154eedc9d3fc0c214a4e5d32bd\",\n",
    "    \"f4c4db3df4ff0de90f44b027fc2e28c16bf7e5c75ea75b0a9762bbb7ac86e7a3\",\n",
    "    \"4193474b2f1c72f735b13633b219d9cabdd43c21d9c2bb4dfc4809f104ba4c06\",\n",
    "    \"f73e37957c74f554be132986f38b6f1d75339f636dfe2b681a0cf3f88d2733af\",\n",
    "    \"a4c44fc5f5bf213e2be6091ccaed49d8bf039d78f6fbd9c4d7b7428cfcb2eda4\",\n",
    "    \"cab4875269f44a701c5e58190a1d2f6fcb577ea79d842522dcab20ccb39b7ad2\",\n",
    "    \"8ecdb93582b2d5270457b36651b62776256ade3aaa2d7432ae65c14f07432d49\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     6\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 6\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                2.0\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_class_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               200\n",
      "MEAN_PIXEL                     [43.53 39.56 48.22]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           nucleus\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         1000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.9\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    64\n",
      "STEPS_PER_EPOCH                105\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               4\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NucleusConfig(Config):\n",
    "    \"\"\"Configuration for training on the nucleus segmentation dataset.\"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"nucleus\"\n",
    "\n",
    "    # Adjust depending on your GPU memory\n",
    "    IMAGES_PER_GPU = 6\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + nucleus\n",
    "\n",
    "    # Number of training and validation steps per epoch\n",
    "    STEPS_PER_EPOCH = (657 - len(VAL_IMAGE_IDS)) // IMAGES_PER_GPU\n",
    "    VALIDATION_STEPS = max(1, len(VAL_IMAGE_IDS) // IMAGES_PER_GPU)\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between nucleus and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet50\"\n",
    "\n",
    "    # Input image resizing\n",
    "    # Random crops of size 512x512\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_MIN_SCALE = 2.0\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 200\n",
    "\n",
    "    # Max number of final detections per image\n",
    "    DETECTION_MAX_INSTANCES = 400\n",
    "\n",
    "\n",
    "class NucleusInferenceConfig(NucleusConfig):\n",
    "    # Set batch size to 1 to run one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    # Don't resize imager for inferencing\n",
    "    IMAGE_RESIZE_MODE = \"pad64\"\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "    \n",
    "config = NucleusConfig()\n",
    "config.display()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleusDataset(utils.Dataset):\n",
    "\n",
    "    def load_nucleus(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the nuclei dataset.\n",
    "\n",
    "        dataset_dir: Root directory of the dataset\n",
    "        subset: Subset to load. Either the name of the sub-directory,\n",
    "                such as stage1_train, stage1_test, ...etc. or, one of:\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: validation images from VAL_IMAGE_IDS\n",
    "        \"\"\"\n",
    "        # Add classes. We have one class.\n",
    "        # Naming the dataset nucleus, and the class nucleus\n",
    "        self.add_class(\"nucleus\", 1, \"nucleus\")\n",
    "\n",
    "        # Which subset?\n",
    "        # \"val\": use hard-coded list above\n",
    "        # \"train\": use data from stage1_train minus the hard-coded list above\n",
    "        # else: use the data from the specified sub-directory\n",
    "        assert subset in [\"train\", \"val\", \"stage1_train\", \"stage1_test\", \"stage2_test\"]\n",
    "        subset_dir = \"stage1_train\" if subset in [\"train\", \"val\"] else subset\n",
    "        dataset_dir = os.path.join(dataset_dir, subset_dir)\n",
    "        if subset == \"val\":\n",
    "            image_ids = VAL_IMAGE_IDS\n",
    "        else:\n",
    "            # Get image ids from directory names\n",
    "            image_ids = next(os.walk(dataset_dir))[1]\n",
    "            if subset == \"train\":\n",
    "                image_ids = list(set(image_ids) - set(VAL_IMAGE_IDS))\n",
    "\n",
    "        # Add images\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                \"nucleus\",\n",
    "                image_id=image_id,\n",
    "                path=os.path.join(dataset_dir, image_id, \"images/{}.png\".format(image_id)))\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        # Get mask directory from image path\n",
    "        mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n",
    "\n",
    "        # Read mask files from .png image\n",
    "        mask = []\n",
    "        for f in next(os.walk(mask_dir))[2]:\n",
    "            if f.endswith(\".png\"):\n",
    "                m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)\n",
    "                mask.append(m)\n",
    "        mask = np.stack(mask, axis=-1)\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID, we return an array of ones\n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"nucleus\":\n",
    "            return info[\"id\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ada9f75b7eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNucleusDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_nucleus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dir' is not defined"
     ]
    }
   ],
   "source": [
    " # Training dataset.\n",
    "dataset_train = NucleusDataset()\n",
    "dataset_train.load_nucleus(dataset_dir, subset)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "# dataset_val = NucleusDataset()\n",
    "# dataset_val.load_nucleus(dataset_dir, \"val\")\n",
    "# dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
